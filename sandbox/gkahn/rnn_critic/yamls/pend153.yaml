exp_name: pend153
exp_prefix: rnn_critic
seed: 3

#################
### Algorithm ###
#################

alg:
  env: GymEnv('Pendulum-v0', record_video=False, force_reset=False)
  env_eval: GymEnv('Pendulum-v0', record_video=False, force_reset=True, video_schedule=FixedIntervalVideoSchedule(10))
  n_envs: 1
  
  total_steps: 4.e+4
  sample_after_n_steps: -1
  onpolicy_after_n_steps: -1
  learn_after_n_steps: 1.e+4
  train_every_n_steps: 1
  eval_every_n_steps: 5.e+2
  save_every_n_steps: 1.e+4
  update_target_after_n_steps: 1.e+3
  update_target_every_n_steps: 1.e+3
  update_preprocess_every_n_steps: 1.e+3
  log_every_n_steps: 1.e+3

  exploration_strategy:
    class: GaussianStrategy
    GaussianStrategy:
      endpoints: [[0, 1], [1.e+5, 0.1], [2.e+5, 0.01]]
      outside_value: 0.01
    OUStrategy:
      mu: 0
      theta: 0.15
      sigma: 0.2
    
  batch_size: 32
  replay_pool_size: 2.e+5
  save_rollouts: True
  save_rollouts_observations: True
  render: False

##############
### Policy ###
##############

policy:
  N: 10
  H: 1
  gamma: 0.99
  obs_history_len: 1

  values_softmax: mean # mean / final / learned
  use_target: True
  separate_target_params: True
  
  get_action_test:
    H: 1
    values_softmax: mean # mean / final / learned
    type: random # random / lattice
    random:
      K: 10000
    lattice:
    beam:
      K: 1000
      M: 10
      
  get_action_target:
    H: 1
    values_softmax: mean # mean / final / learned
    type: random # random / lattice
    random:
      K: 100
    lattice:
    beam:
      K: 10
      M: 10

  class: CDQNPolicy
  CDQNPolicy:
    obs_hidden_layers: [128]
    value_hidden_layers: [128, 16]
    use_bilinear: True
    activation: tf.nn.tanh
  MACPolicy:
    obs_hidden_layers: [128]
    action_hidden_layers: []
    reward_hidden_layers: [16]
    value_hidden_layers: [16]
    lambda_hidden_layers: [16]
    rnn_state_dim: 128
    use_lstm: False
    use_bilinear: True
    activation: tf.nn.tanh
    rnn_activation: tf.nn.relu

  # preprocessing
  preprocess:
    observations_mean: False
    observations_orth: False
    actions_mean: False
    actions_orth: False
    rewards_mean: False
    rewards_orth: False

  # training
  weight_decay: 1.e-7
  lr_schedule:
    endpoints: [[0, 1.e-3], [1.e+6, 1.e-3]]
    outside_value: 1.e-3
  grad_clip_norm: 10

  # device
  gpu_device: 7
  gpu_frac: 0.4


