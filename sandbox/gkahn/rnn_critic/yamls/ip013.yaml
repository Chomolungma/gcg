exp_name: ip013
exp_prefix: rnn_critic
seed: 1

#################
### Algorithm ###
#################

alg:
  env: GymEnv('InvertedPendulum-v1', record_video=False)
  n_envs: 1

  total_steps: 2.e+5
  learn_after_n_steps: 1.e+3
  train_every_n_steps: 4
  save_every_n_steps: 4.e+4
  update_target_after_n_steps: 1.e+3
  update_target_every_n_steps: 1.e+3
  update_preprocess_every_n_steps: 1.e+3
  log_every_n_steps: 1.e+3

  exploration_strategy:
    class: GaussianStrategy
    GaussianStrategy:
      endpoints: [[0, 0.5], [1.e+3, 0.5], [1.e+4, 0.1]]
      outside_value: 0.01
    EpsilonGreedyStrategy:
      endpoints: [[0, 0.5], [5.e+4, 0.5], [5.e+5, 0.01], [1.e+6, 0.01]]
      outside_value: 0.01
    
  batch_size: 32
  replay_pool_size: 1.e+6
  save_rollouts: True
  render: False

##############
### Policy ###
##############

policy:
  N: 3
  H: 3
  cost_type: combined # combined / separated
  gamma: 0.99
  obs_history_len: 1
  use_target: False
  separate_target_params: True

  class: MultiactionCombinedcostRNNPolicy
  DQNPolicy:
    hidden_layers: [128, 128]
    activation: tf.nn.tanh
    concat_or_bilinear: bilinear
  DiscreteDQNPolicy:
    hidden_layers: [512]
    activation: tf.nn.relu
    conv_hidden_layers: [32, 64, 64]
    conv_kernels: [8, 4, 3]
    conv_strides: [4, 2, 1]
    conv_activation: tf.nn.relu
  NstepDQNPolicy:
    hidden_layers: [128, 128]
    activation: tf.nn.tanh
    concat_or_bilinear: bilinear
  NstepDiscreteDQNPolicy:
    hidden_layers: [512]
    activation: tf.nn.relu
    conv_hidden_layers: [32, 64, 64]
    conv_kernels: [8, 4, 3]
    conv_strides: [4, 2, 1]
    conv_activation: tf.nn.relu
  MultiactionCombinedcostMLPPolicy:
    hidden_layers: [128, 128]
    activation: tf.nn.tanh
    concat_or_bilinear: bilinear
  MultiactionCombinedcostRNNPolicy:
    obs_hidden_layers: [32]
    action_hidden_layers: [32]
    reward_hidden_layers: [16]
    rnn_state_dim: 32
    use_lstm: True
    activation: tf.nn.relu
    rnn_activation: tf.nn.relu
  MultiactionCombinedcostMuxRNNPolicy:
    obs_hidden_layers: [512]
    reward_hidden_layers: [16]
    rnn_state_dim: 32
    use_lstm: True
    activation: tf.nn.relu
    rnn_activation: tf.nn.relu
    conv_hidden_layers: [32, 64, 64]
    conv_kernels: [8, 4, 3]
    conv_strides: [4, 2, 1]
    conv_activation: tf.nn.relu
  MultiactionSeparatedcostMLPPolicy:
    hidden_layers: [128, 128]
    activation: tf.nn.tanh
    concat_or_bilinear: bilinear
  MultiactionSeparatedcostRNNPolicy:
    obs_hidden_layers: [64,64]
    action_hidden_layers: [64,64]
    reward_hidden_layers: [64,64]
    value_hidden_layers: [64,64]
    rnn_state_dim: 64
    use_lstm: True
    activation: tf.nn.relu
    rnn_activation: tf.nn.relu

  # preprocessing
  preprocess:
    observations_mean: True
    observations_orth: True
    actions_mean: False
    actions_orth: False
    rewards_mean: False
    rewards_orth: False

  # training
  weight_decay: 1.e-9
  lr_schedule:
    endpoints: [[0, 1.e-3], [1.e+6, 1.e-3]]
    outside_value: 1.e-3
  grad_clip_norm: 10

  # device
  gpu_device: 0
  gpu_frac: 0.2

########################
### Action selection ###
########################

get_action:
  type: random # random / lattice

  random:
    type: random
    K: 1000
  lattice:
    type: lattice
    K: 10
    
get_target_action:
  type: random # random / lattice

  random:
    type: random
    K: 100
  lattice:
    type: lattice
    K: 10

